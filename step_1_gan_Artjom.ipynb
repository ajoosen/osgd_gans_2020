{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the distribution that we will learn\n",
    "data_mean = 10\n",
    "data_stddev =  math.sqrt(2)\n",
    "series_length = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the shape of the generator\n",
    "\n",
    "Takes 20 random inputs and generates a distribution (as defined above)\n",
    "Hidden layer neuron count is defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator shape\n",
    "g_input_size = 20    \n",
    "\n",
    "#hidden layer neuron count\n",
    "g_hidden_size = 150  \n",
    "\n",
    "g_output_size = series_length  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator shape\n",
    "\n",
    "d_input_size = series_length\n",
    "d_hidden_size = 75   \n",
    "\n",
    "#output is single value where true = 1 (matches distribution) and fake = 0 (does not match distribution)\n",
    "d_output_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define two functions to return functions that provide\n",
    "a true sample and some random noise. The true sample\n",
    "trains the discriminator, the random noise feeds the \n",
    "generator.\n",
    "\n",
    "Make local copies of the signal generator functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_sampler(mu, sigma):\n",
    "    dist = Normal( mu, sigma )\n",
    "    return lambda m, n: dist.sample( (m, n) ).requires_grad_()\n",
    "\n",
    "def get_noise_sampler():\n",
    "    return lambda m, n: torch.rand(m, n).requires_grad_()  # Uniform-dist data into generator, _NOT_ Gaussian\n",
    "\n",
    "actual_data = get_real_sampler( data_mean, data_stddev )\n",
    "noise_data  = get_noise_sampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the generator \n",
    "\n",
    "It's important the generator can output means that match. Be careful\n",
    "using something like sigmoid, which outputs 0..1. That would not learn\n",
    "something with mean 2.0 !\n",
    "\n",
    "This network takes in noise\n",
    "and produces an output.\n",
    "\n",
    "xfer is the transfer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        #instantiate nn.Module object because we inherit from it (?)\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        #define linear layers of give in- and output size \n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map6 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        #transfer function - Scaled Exponential Linear Unit (SELU)\n",
    "        #selu works ok\n",
    "        self.xfer = torch.nn.SELU()\n",
    "        \n",
    "        #relu doesn't converge\n",
    "        #self.xfer = torch.nn.ReLU()\n",
    "        \n",
    "        #leakyReLU doesn't converge\n",
    "        #self.xfer=torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #forward pass -- \n",
    "        x = self.xfer(self.map1(x))\n",
    "        x = self.xfer(self.map2(x))\n",
    "        x = self.xfer(self.map3(x))\n",
    "        x = self.xfer(self.map4(x))\n",
    "        x = self.xfer(self.map5(x))\n",
    "        return self.xfer(self.map6(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the last layer should restrict to 0..1 (opposite of the generator)\n",
    "This allows us more choice in loss functions.\n",
    "\n",
    "This network is a classic multilayer perceptron - really nothing\n",
    "special at all. It returns true/false based on the learned\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map4 = nn.Linear(hidden_size, output_size)\n",
    "        self.elu = torch.nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.map1(x))\n",
    "        x = self.elu(self.map2(x))\n",
    "        x = self.elu(self.map3(x))\n",
    "        return torch.sigmoid( self.map4(x) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "\n",
    "D = Discriminator(input_size=d_input_size, hidden_size=d_hidden_size, output_size=d_output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function and optimiser object for generator and discriminator and learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "# d_learning_rate = 5e-4\n",
    "# g_learning_rate = 5e-3\n",
    "d_learning_rate = 3e-3\n",
    "g_learning_rate = 1e-2\n",
    "\n",
    "d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate, momentum=0) #, betas=optim_betas)\n",
    "g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate, momentum=0 ) #, betas=optim_betas)\n",
    "\n",
    "\n",
    "\n",
    "#Adam turns out not to converge...(?????)\n",
    "# d_optimizer = torch.optim.Adam(D.parameters(), lr=d_learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "# g_optimizer = torch.optim.Adam(G.parameters(), lr=g_learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fires in real data and learns a 1.0 mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_D_on_actual() :\n",
    "    real_data = actual_data(d_minibatch_size, d_input_size)\n",
    "    decision = D(real_data)\n",
    "    error = criterion(decision, torch.ones(d_minibatch_size, 1))  # ones = true\n",
    "    error.backward() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other side of training: learn to recognize fake output from\n",
    "the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_D_on_generated() :\n",
    "    noise = noise_data(d_minibatch_size, g_input_size)\n",
    "    \n",
    "    #to get fake data simply pass noise into the generator \n",
    "    fake_data = G(noise) \n",
    "    decision = D(fake_data)\n",
    "    \n",
    "    error = criterion(decision, torch.zeros(d_minibatch_size, 1))  # zeros = fake\n",
    "    error.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training for the generator is to assume that the generator\n",
    "produces perfect data (i.e. the discriminator returns 1.0).\n",
    "Then learn how to improve the output from the generator based on the discriminators actual output.\n",
    "\n",
    "This is the key piece of a GAN:\n",
    "pass the error through both networks, but only update the generators\n",
    "weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_G():\n",
    "    noise = noise_data(g_minibatch_size, g_input_size)\n",
    "    fake_data = G(noise)\n",
    "    fake_decision = D(fake_data)\n",
    "    error = criterion(fake_decision, torch.ones(g_minibatch_size, 1) )  # we want to fool, so pretend it's all genuine\n",
    "\n",
    "    error.backward()\n",
    "    return error.item(), fake_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop, train based on loads of data.\n",
    "\n",
    "The algo works like this:\n",
    "    \n",
    "Step 1 is plain old batch learning, if the rest of the code were\n",
    "removed you would have a network that can identify the desired distribution\n",
    "\n",
    "- train the discriminator just like you would train any network\n",
    "- use both true and false (generated) samples to learn\n",
    "\n",
    "Step 2 is the GAN difference\n",
    "- train the generator to produce, but don't compare the output to a good sample\n",
    "- feed the sample generated output through the disciminator to spot the fake\n",
    "- backpropagate the error through the discriminator and the generator\n",
    "\n",
    "So let's think about the possible cases (in *all* cases only the generators parameters \n",
    "are updated in the step 2)\n",
    "\n",
    "*Discrimator perfect, Generator Perfect*\n",
    "Generator makes a sample which is identified as 1.0. Error is 0.0, no learning occurs\n",
    "\n",
    "*Discrimator perfect, Generator Rubbish*\n",
    "Generator makes noise which is identified as 0.0. Error is 1.0, error is propagated and the generator learns\n",
    "\n",
    "*Discrimator rubbish, Generator Perfect*\n",
    "Generator makes sample which is identified as 0.0. Error is 1.0, error is propagated the generator would not learn much because the error would be absorbed by the discriminator\n",
    "\n",
    "*Discrimator rubbish, Generator Rubbish*\n",
    "Generator makes sample which is identified as 0.5. Error is 0.5, error is propagated the gradients in the discriminator and generator would mean the error is shared between both\n",
    "and learning occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    400,    Loss = 1.474\n",
      "Epoch    800,    Loss = 3.230\n",
      "Epoch   1200,    Loss = 4.368\n",
      "Epoch   1600,    Loss = 2.201\n",
      "Epoch   2000,    Loss = 3.276\n"
     ]
    }
   ],
   "source": [
    "# Now define how to send data into the process\n",
    "\n",
    "# There are two phases of training, train the discriminator then the generator. \n",
    "# It seems to be important that the discriminator be 'better' than the generator.\n",
    "# Sometimes more batches are sent to train the discriminator than the generator.\n",
    "# In this case we put more stuff into the discriminator training.\n",
    "\n",
    "\n",
    "d_minibatch_size = 15 \n",
    "g_minibatch_size = 10\n",
    "\n",
    "num_epochs = 4000\n",
    "print_interval = num_epochs/10\n",
    "\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #discriminator\n",
    "    D.zero_grad()\n",
    "    \n",
    "    train_D_on_actual()    \n",
    "    train_D_on_generated()\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    #generator\n",
    "    G.zero_grad()\n",
    "    loss, generated = train_G()\n",
    "    g_optimizer.step()\n",
    "    \n",
    "    losses.append(loss)\n",
    "    if( epoch % print_interval) == (print_interval-1) :\n",
    "        print(\"Epoch %6d,    Loss = %5.3f\" % (epoch+1, loss))\n",
    "        \n",
    "    relative_change = (losses[epoch] - losses[epoch-1])/losses[epoch-1]\n",
    "    \n",
    "    if (relative_change <= 10^-2):\n",
    "        print('Converged at epoch ' +str(epoch))\n",
    "        break\n",
    "        \n",
    "print( \"Training complete\" )\n",
    "# print(generated)\n",
    "# print(generated.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(len(losses))\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(x,losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(bottom=0)\n",
    "plt.title('Loss vs. Epochs \\n data mean = '+str(data_mean)+', data st dev = '+str(round(data_stddev,2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if what the GAN generates is accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_length = 100\n",
    "\n",
    "#the 'generated' tensor contains the last minibatch of 10 generated samples\n",
    "\n",
    "d = torch.empty(generated.size(0), demo_length) \n",
    "\n",
    "#print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw( data ) :    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    \n",
    "    #generated distributions\n",
    "    generated_distributions = data.tolist() if isinstance(data, torch.Tensor) else data\n",
    "    plt.plot(generated_distributions)\n",
    "    \n",
    "    #plt.xticks(ticks = [1, 2, 3], labels = ['0', '1', '2'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, d.size(0)) :\n",
    "    d[i] = torch.histc(generated[i], min=0, max=data_mean+3*data_stddev, bins=demo_length)\n",
    "\n",
    "# .t() method transposes the 2 dimensional tensor\n",
    "draw( d.t() )\n",
    "\n",
    "#plotting normal distribution with correct parameters for reference\n",
    "original=torch.empty(demo_length).normal_(mean=data_mean,std=data_stddev)\n",
    "original_histogram = torch.histc( original, min=0, max=data_mean+3*data_stddev, bins=demo_length )\n",
    "plt.plot(original_histogram, linewidth=4, label='samples directly from normal distribution',color='blue')\n",
    "plt.legend()\n",
    "plt.title('Last minibatch of 10 generated samples vs. real')\n",
    "plt.xlabel('histogram bins')\n",
    "plt.ylabel('frequency')\n",
    "plt.xlim((0, demo_length)) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.mean(generated, dim=0)\n",
    "std_devs = mean.std(dim=0)\n",
    "\n",
    "avg_mean = torch.mean(mean, dim=0)\n",
    "mean_std_dev = std_devs.mean(dim=0)\n",
    "\n",
    "print('GAN:      mean = ' + str(round(avg_mean.item(),3)) +    '    |  st dev = ' + str(round(mean_std_dev.item(),3)))\n",
    "print('REAL:     mean = ' + str(data_mean)                + '      |  st dev = ' + str(round(data_stddev,3)))\n",
    "print('% ERROR:         ' + str( round(100*((avg_mean.item()-data_mean)/data_mean),2) ) +'%   |          '+str( round(100*((mean_std_dev.item()-data_stddev)/data_stddev),1) ) +'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
